{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a977cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brimmann/works/llm-recipes-2/.venv/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/brimmann/works/llm-recipes-2/.venv/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/brimmann/works/llm-recipes-2/models/checkpoint_handler.py:7: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n",
      "  import torch.distributed._shard.checkpoint as dist_cp\n",
      "/home/brimmann/works/llm-recipes-2/models/tools.py:5: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging\n"
     ]
    }
   ],
   "source": [
    "from train.train_utils import train\n",
    "from configs.configs_utils import update_config\n",
    "from data.data_utils import (get_dataloader, get_distillation_dataloader)\n",
    "from train.tools import (setup, setup_environ_flags, clear_gpu_cache)\n",
    "from models.models_utils import  get_optimizer\n",
    "from models.xrag_models_utils import get_xrag_models\n",
    "from data.data_utils import get_dataloader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f77a4b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.datasets import dataset as DatasetConfig\n",
    "from configs.training import train_config as TrainConfig\n",
    "from configs.distillation import distillation_config as DistillationConfig\n",
    "from configs.fsdp import fsdp_config as FsdpConfig\n",
    "\n",
    "dataset_config = DatasetConfig()\n",
    "train_config = TrainConfig()\n",
    "distill_config = DistillationConfig()\n",
    "fsdp_config = DistillationConfig()\n",
    "train_config.model_name = \"google/gemma-3-1b-it\"\n",
    "distill_config.model_name = \"Hannibal046/xrag-7b\"\n",
    "train_config.batch_size_training = 8\n",
    "train_config.distillation = True\n",
    "train_config.num_workers_dataloader = 0\n",
    "train_config.num_epochs = 5\n",
    "dataset_config.file = \"data/loaders/squad-v2-sampled.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ba61cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712e6300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38505528e460437db6b7724274315a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XGemmaForCausalLM were not initialized from the model checkpoint at google/gemma-3-1b-it and are newly initialized: ['projector.projector.0.bias', 'projector.projector.0.weight', 'projector.projector.2.bias', 'projector.projector.2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters in student model:\n",
      "projector.projector.0.weight\n",
      "projector.projector.0.bias\n",
      "projector.projector.2.weight\n",
      "projector.projector.2.bias\n"
     ]
    }
   ],
   "source": [
    "student_tokenizer, teacher_tokenizer, model = get_xrag_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c353f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_train_dataloader, teacher_eval_dataloader = get_dataloader(dataset_config, train_config, teacher_tokenizer, rank, distill_config)\n",
    "student_train_dataloader, student_eval_dataloader = get_dataloader(dataset_config, train_config, student_tokenizer, rank, distill_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56048ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model, train_config, fsdp_config)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=train_config.lr, epochs=train_config.num_epochs, steps_per_epoch=len(student_train_dataloader),\n",
    "                                                pct_start=train_config.pct_start, div_factor=train_config.div_factor, final_div_factor=train_config.final_div_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1918de",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_batch_teacher_train_dataloader = next(iter(teacher_train_dataloader))\n",
    "singel_batch_student_student_train_dataloader = next(iter(student_train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8520012",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train(\n",
    "      model,\n",
    "      single_batch_teacher_train_dataloader,\n",
    "      singel_batch_student_student_train_dataloader,\n",
    "      optimizer,\n",
    "      scheduler,\n",
    "      train_config.gradient_accumulation_steps,\n",
    "      train_config,\n",
    "      distill_config,\n",
    "      dataset_config,\n",
    "      teacher_train_dataloader if train_config.distillation else None,\n",
    "      teacher_eval_dataloader if train_config.distillation else None,\n",
    "      fsdp_config if train_config.enable_fsdp else None,\n",
    "      None,\n",
    "      rank,\n",
    "  )\n",
    "if rank == 0:\n",
    "    [print(f'Key: {k}, Value: {v}') for k, v in results.items()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
